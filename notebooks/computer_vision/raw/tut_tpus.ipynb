{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction #\n",
    "\n",
    "Welcome to the [**Petals to the Metal**](https://www.kaggle.com/c/tpu-getting-started) competition! In this competition, youâ€™re challenged to build a machine learning model that identifies the type of flowers in a dataset of images.\n",
    "\n",
    "In this tutorial notebook, you'll learn how to build an image classifier in Keras and train it on a [Tensor Processing Unit (TPU)](https://www.kaggle.com/docs/tpu). Then, in the [following exercise](#$NEXT_NOTEBOOK_URL$), you'll create your *own* notebook and make a submission to the competition. At the end, you'll have a complete project you can build off of with ideas of your own.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Get Started Now!</strong><br>\n",
    "You don't need to understand everything in this notebook before starting the exercise. If you like, go ahead and <b><a href=\"#$NEXT_NOTEBOOK_URL$\">open the exercise</a></b> and follow the instructions at the beginning to enter the competition. Then, return to this tutorial for a walkthrough of your starter project.\n",
    "</blockquote>\n",
    "\n",
    "# Load the Helper Functions #\n",
    "\n",
    "Attached to the notebook is a utility script called [`petal_helper`](https://www.kaggle.com/ryanholbrook/petal-helper). It contains a number of helper functions related to data loading and visualization. The following cell will import them into your notebook session. We'll also import TensorFlow, which we'll use to create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from petal_helper import *\n",
    "\n",
    "import tensorflow as tf\n"
    "import tf_keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Strategy #\n",
    "\n",
    "A TPU has eight different *cores* and each of these cores acts as its own accelerator. (A TPU is sort of like having eight GPUs in one machine.) We tell TensorFlow how to make use of all these cores at once through a **distribution strategy**. Run the following cell to create the distribution strategy that we'll later apply to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Detect TPU, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will distribute training among the eight TPU cores by creating eight different *replicas* of your model. \n",
    "\n",
    "# Loading the Competition Data #\n",
    "\n",
    "When used with TPUs, datasets are often serialized into [TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord). This is a format convenient for distributing data to each of the TPUs cores.\n",
    "\n",
    "We've included functions in the `petal_helper` script that will load the TFRecords for you and create a data pipeline you can use with your model. There is one function for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = get_training_dataset()\n",
    "ds_valid = get_validation_dataset()\n",
    "ds_test = get_test_dataset()\n",
    "\n",
    "print(\"Training:\", ds_train)\n",
    "print (\"Validation:\", ds_valid)\n",
    "print(\"Test:\", ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are `tf.data.Dataset` objects. You can think about a dataset in TensorFlow as a *stream* of data records. Check out [this guide](https://www.tensorflow.org/guide/data) for more on working with the `tf.data` API.\n",
    "\n",
    "# Define Model #\n",
    "\n",
    "Now we're ready to create a neural network for classifying images! We'll use what's known as **transfer learning**. With transfer learning, you reuse part of a pretrained model to get a head-start on a new dataset.\n",
    "\n",
    "For this tutorial, we'll to use a model called **VGG16** pretrained on [ImageNet](http://image-net.org/)). Later, you might want to experiment with [other models](https://www.tensorflow.org/api_docs/python/tf/keras/applications) included with Keras. ([Xception](https://www.tensorflow.org/api_docs/python/tf/keras/applications/Xception) wouldn't be a bad choice.)\n",
    "\n",
    "The distribution strategy we created earlier contains a [context manager](https://docs.python.org/3/reference/compound_stmts.html#with), `strategy.scope`. This context manager tells TensorFlow how to divide the work of training among the eight TPU cores. When using TensorFlow with a TPU, it's important to define your model in a `strategy.scope()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf_keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False ,\n",
    "        input_shape=[*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    model = tf_keras.Sequential([\n",
    "        # To a base pretrained on ImageNet to extract features from images...\n",
    "        pretrained_model,\n",
    "        # ... attach a new head to act as a classifier.\n",
    "        tf_keras.layers.GlobalAveragePooling2D(),\n",
    "        tf_keras.layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'],\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #\n",
    "\n",
    "And now we're ready to train the model! After defining a few parameters, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "# Define training epochs\n",
    "EPOCHS = 12\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell shows how the loss and metrics progressed during training. Thankfully, it converges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_curves(\n",
    "    history.history['loss'],\n",
    "    history.history['val_loss'],\n",
    "    'loss',\n",
    "    211,\n",
    ")\n",
    "display_training_curves(\n",
    "    history.history['sparse_categorical_accuracy'],\n",
    "    history.history['val_sparse_categorical_accuracy'],\n",
    "    'accuracy',\n",
    "    212,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions #\n",
    "\n",
    "Once you're satisfied with everything, you're ready to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_test_dataset(ordered=True)\n",
    "\n",
    "print('Computing predictions...')\n",
    "test_images_ds = test_ds.map(lambda image, idnum: image)\n",
    "probabilities = model.predict(test_images_ds)\n",
    "predictions = np.argmax(probabilities, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a file `submission.csv`. This file is what you'll submit to get your score on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating submission.csv file...')\n",
    "\n",
    "# Get image ids from test set and convert to unicode\n",
    "test_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\n",
    "test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n",
    "\n",
    "# Write the submission file\n",
    "np.savetxt(\n",
    "    'submission.csv',\n",
    "    np.rec.fromarrays([test_ids, predictions]),\n",
    "    fmt=['%s', '%d'],\n",
    "    delimiter=',',\n",
    "    header='id,label',\n",
    "    comments='',\n",
    ")\n",
    "\n",
    "# Look at the first few predictions\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn! #\n",
    "\n",
    "Now that you've seen what a typical TPU project looks like, you're ready to **[create your own](#$NEXT_NOTEBOOK_URL$)**! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
