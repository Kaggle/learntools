{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<!--TITLE: Convolution and ReLU-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def show_kernel(kernel, label=True, digits=None, text_size=28):\n",
    "    # Format kernel\n",
    "    kernel = np.array(kernel)\n",
    "    if digits is not None:\n",
    "        kernel = kernel.round(digits)\n",
    "\n",
    "    # Plot kernel\n",
    "    cmap = plt.get_cmap('Blues_r')\n",
    "    plt.imshow(kernel, cmap=cmap)\n",
    "    rows, cols = kernel.shape\n",
    "    thresh = (kernel.max()+kernel.min())/2\n",
    "    # Optionally, add value labels\n",
    "    if label:\n",
    "        for i, j in product(range(rows), range(cols)):\n",
    "            val = kernel[i, j]\n",
    "            color = cmap(0) if val > thresh else cmap(255)\n",
    "            plt.text(j, i, val, \n",
    "                     color=color, size=text_size,\n",
    "                     horizontalalignment='center', verticalalignment='center')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "In the last lesson, we saw that a convolutional classifier has two parts: a convolutional **base** and a **head** of dense layers. We learned that the job of the base is to extract visual features from an image, which the head would then use to classify the image.\n",
    "\n",
    "Over the next few lessons, we're going to learn about the two most important types of layers that you'll usually find in the base of a convolutional image classifier. These are the **convolutional layer** with **ReLU activation**, and the **maximum pooling layer**. In Lesson 5, you'll learn how to design your own convnet by composing these layers into blocks that perform the feature extraction.\n",
    "\n",
    "This lesson is about the convolutional layer with its ReLU activation function.\n",
    "\n",
    "# Feature Extraction #\n",
    "\n",
    "Before we get into the details of convolution, let's discuss the *purpose* of these layers in the network. We're going to see how these three operations (convolution, ReLU, and maximum pooling) are used to implement the feature extraction process.\n",
    "\n",
    "The **feature extraction** performed by the base consists of **three basic operations**:\n",
    "1. **Filter** an image for a particular feature (convolution)\n",
    "2. **Detect** that feature within the filtered image (ReLU)\n",
    "3. **Condense** the image to enhance the features (maximum pooling)\n",
    "\n",
    "The next figure illustrates this process. You can see how these three operations are able to isolate some particular characteristic of the original image (in this case, horizontal lines).\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/2-show-extraction.png\" width=\"1200\" alt=\"An example of the feature extraction process.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/IYO9lqp.png\" width=\"600\" alt=\"An example of the feature extraction process.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The three steps of feature extraction.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Typically, the network will perform several extractions in parallel on a single image. In modern convnets, it's not uncommon for the final layer in the base to be producing over 1000 unique visual features.\n",
    "\n",
    "# Filter with Convolution #\n",
    "\n",
    "A convolutional layer carries out the filtering step. You might define a convolutional layer in a Keras model something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3), # activation is None\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand these parameters by looking at their relationship to the *weights* and *activations* of the layer. Let's do that now.\n",
    "\n",
    "## Weights ##\n",
    "\n",
    "The **weights** a convnet learns during training are primarily contained in its convolutional layers. These weights we call **kernels**. We can represent them as small arrays:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel.png\" width=\"150\" alt=\"A 3x3 kernel.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/uJfD9r9.png\" width=\"150\" alt=\"A 3x3 kernel.\">\n",
    "</figure>\n",
    "\n",
    "A kernel operates by scanning over an image and producing a *weighted sum* of pixel values. In this way, a kernel will act sort of like a polarized lens, emphasizing or deemphasizing certain patterns of information.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-kernel-lens.png\" width=\"400\" alt=\"A kernel acts as a kind of lens.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/j3lk26U.png\" width=\"250\" alt=\"A kernel acts as a kind of lens.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A kernel acts as a kind of lens.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "Kernels define how a convolutional layer is connected to the layer that follows. The kernel above will connect each neuron in the output to nine neurons in the input. By setting the dimensions of the kernels with `kernel_size`, you are telling the convnet how to form these connections. Most often, a kernel will have odd-numbered dimensions -- like `kernel_size=(3, 3)` or `(5, 5)` -- so that a single pixel sits at the center, but this is not a requirement.\n",
    "\n",
    "The kernels in a convolutional layer determine what kinds of features it creates. During training, a convnet tries to learn what features it needs to solve the classification problem. This means finding the best values for its kernels.\n",
    "\n",
    "<!--TODO: Learning a kernel-->\n",
    "\n",
    "## Activations ##\n",
    "\n",
    "The **activations** in the network we call **feature maps**. They are what result when we apply a filter to an image; they contain the visual features the kernel extracts. Here are a few kernels pictured with feature maps they produced.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/JxBwchH.png\" width=\"800\" alt=\"Three kernels and the feature maps they produce.\"><figcaption style=\"textalign: center; font-style: italic\"><center>Kernels and features.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "From the pattern of numbers in the kernel, you can tell the kinds of feature maps it creates. Generally, what a convolution accentuates in its inputs will match the shape of the *positive* numbers in the kernel. The left and middle kernels above will both filter for horizontal shapes.\n",
    "\n",
    "With the `filters` parameter, you tell the convolutional layer how many feature maps you want it to create as output.\n",
    "\n",
    "# Detect with ReLU #\n",
    "\n",
    "After filtering, the feature maps pass through the activation function. The **rectifier function** has a graph like this:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/relu.png\" width=\"300\" alt=\"\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/DxGJuTH.png\" width=\"300\" alt=\"Graph of the ReLU activation function.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The graph of the rectifier function looks like a line with the negative part \"rectified\" to 0.</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "A neuron with a rectifier attached is called a *rectified linear unit*. For that reason, we might also call the rectifier function the **ReLU activation** or even the ReLU function.\n",
    "\n",
    "The ReLU activation can be defined in its own `Activation` layer, but most often you'll just include it as the activation function of `Conv2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu')\n",
    "    # More layers follow\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could think about the activation function as scoring pixel values according to some measure of importance. The ReLU activation says that negative values are not important and so sets them to 0. (\"Everything unimportant is equally unimportant.\")\n",
    "\n",
    "Here is ReLU applied the feature maps above. Notice how it succeeds at isolating the features.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/3-relu-and-maps.png\" width=\"800\" alt=\"ReLU applied to feature maps.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/dKtwzPY.png\" width=\"800\" alt=\"ReLU applied to feature maps.\">\n",
    "</figure>\n",
    "\n",
    "Like other activation functions, the ReLU function is **nonlinear**. Essentially this means that the total effect of all the layers in the network becomes different than what you would get by just adding the effects together -- which would be the same as what you could achieve with only a single layer. The nonlinearity ensures features will combine in interesting ways as they move deeper into the network. (We'll explore this \"feature compounding\" more in Lesson 5.)\n",
    "\n",
    "# Example - Apply Convolution and ReLU #\n",
    "\n",
    "We'll do the extraction ourselves in this example to understand better what convolutional networks are doing \"behind the scenes\".\n",
    "\n",
    "Here is the image we'll use for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "\n",
    "image_path = '../input/computer-vision-resources/car_feature.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.io.decode_jpeg(image)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the filtering step, we'll define a kernel and then apply it with the convolution. The kernel in this case is an \"edge detection\" kernel. You can define it with `tf.constant` just like you'd define an array in Numpy with `np.array`. This creates a *tensor* of the sort TensorFlow uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "kernel = tf.constant([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1],\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "show_kernel(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow includes many common operations performed by neural networks in its `tf.nn` [module](https://www.tensorflow.org/api_docs/python/tf/nn). The two that we'll use are `conv2d` and `relu`. These are simply function versions of Keras layers.\n",
    "\n",
    "This next hidden cell does some reformatting to make things compatible with TensorFlow. The details aren't important for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE$\n",
    "# Reformat for batch compatibility.\n",
    "image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "image = tf.expand_dims(image, axis=0)\n",
    "kernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\n",
    "kernel = tf.cast(kernel, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our kernel and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filter = tf.nn.conv2d(\n",
    "    input=image,\n",
    "    filters=kernel,\n",
    "    # we'll talk about these two in lesson 4!\n",
    "    strides=1,\n",
    "    padding='SAME',\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_filter))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the detection step with the ReLU function. This function is much simpler than the convolution, as it doesn't have any parameters to set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_detect = tf.nn.relu(image_filter)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tf.squeeze(image_detect))\n",
    "plt.axis('off')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we've created a feature map! Images like these are what the head uses to solve its classification problem. We can imagine that certain features might be more characteristic of *Cars* and others more characteristic of *Trucks*. The task of a convnet during training is to create kernels that can find those features.\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "We saw in this lesson the first two steps a convnet uses to perform feature extraction: **filter** with `Conv2D` layers and **detect** with `relu` activation.\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "In [**the exercises**](#$NEXT_NOTEBOOK_URL$), you'll have a chance to experiment with the kernels in the pretrained VGG16 model we used in Lesson 1."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
