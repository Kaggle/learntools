{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TITLE:Custom Convnets-->\n",
    "# Introduction #\n",
    "\n",
    "Now that you've seen the layers a convnet uses to extract features, it's time to put them together and build a network of your own!\n",
    "\n",
    "# Simple to Refined #\n",
    "\n",
    "In the last three lessons, we saw how convolutional networks perform **feature extraction** through three operations: **filter**, **detect**, and **condense**. A single round of feature extraction can only extract relatively simple features from an image, things like simple lines or contrasts. These are too simple to solve most classification problems. Instead, convnets will repeat this extraction over and over, so that the features become more complex and refined as they travel deeper into the network.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/VqmC1rm.png\" alt=\"Features extracted from an image of a car, from simple to refined.\" width=800>\n",
    "</figure>\n",
    "\n",
    "# Convolutional Blocks #\n",
    "\n",
    "It does this by passing them through long chains of **convolutional blocks** which perform this extraction.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/pr8VwCZ.png\" width=\"400\" alt=\"Extraction as a sequence of blocks.\">\n",
    "</figure>\n",
    "\n",
    "These convolutional blocks are stacks of `Conv2D` and `MaxPool2D` layers, whose role in feature extraction we learned about in the last few lessons.\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/2-block-crp.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/8D6IhEw.png\" width=\"400\" alt=\"A kind of extraction block: convolution, ReLU, pooling.\">\n",
    "</figure>\n",
    "\n",
    "Each block represents a round of extraction, and by composing these blocks the convnet can combine and recombine the features produced, growing them and shaping them to better fit the problem at hand. The deep structure of modern convnets is what allows this sophisticated feature engineering and has been largely responsible for their superior performance.\n",
    "\n",
    "# Example - Design a Convnet #\n",
    "\n",
    "Let's see how to define a deep convolutional network capable of engineering complex features. In this example, we'll create a Keras `Sequence` model and then train it on our Cars dataset.\n",
    "\n",
    "## Step 1 - Load Data ##\n",
    "\n",
    "This hidden cell loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "# Imports\n",
    "import os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Reproducability\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\") # to clean up output cells\n",
    "\n",
    "\n",
    "# Load training and validation sets\n",
    "ds_train_ = image_dataset_from_directory(\n",
    "    '../input/car-or-truck/train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "ds_valid_ = image_dataset_from_directory(\n",
    "    '../input/car-or-truck/valid',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=[128, 128],\n",
    "    interpolation='nearest',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Data Pipeline\n",
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ds_train = (\n",
    "    ds_train_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "ds_valid = (\n",
    "    ds_valid_\n",
    "    .map(convert_to_float)\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Define Model ##\n",
    "\n",
    "Here is a diagram of the model we'll use:\n",
    "\n",
    "<figure>\n",
    "<!-- <img src=\"./images/2-convmodel-1.png\" width=\"200\" alt=\"Diagram of a convolutional model.\"> -->\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/U1VdoDJ.png\" width=\"250\" alt=\"Diagram of a convolutional model.\">\n",
    "</figure>\n",
    "\n",
    "Now we'll define the model. See how our model consists of three blocks of `Conv2D` and `MaxPool2D` layers (the base) followed by a head of `Dense` layers. We can translate this diagram more or less directly into a Keras `Sequential` model just by filling in the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=5, activation=\"relu\", padding='same',\n",
    "                  # give the input dimensions in the first layer\n",
    "                  # [height, width, color channels(RGB)]\n",
    "                  input_shape=[128, 128, 3]),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=6, activation=\"relu\"),\n",
    "    layers.Dense(units=1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in this definition is how the number of filters doubled block-by-block: 32, 64, 128. This is a common pattern. Since the `MaxPool2D` layer is reducing the *size* of the feature maps, we can afford to increase the *quantity* we create.\n",
    "\n",
    "## Step 3 - Train ##\n",
    "\n",
    "We can train this model just like the model from Lesson 1: compile it with an optimizer along with a loss and metric appropriate for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=40,\n",
    "    verbose=0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is much smaller than the VGG16 model from Lesson 1 -- only 3 convolutional layers versus the 16 of VGG16. It was nevertheless able to fit this dataset fairly well. We might still be able to improve this simple model by adding more convolutional layers, hoping to create features better adapted to the dataset. This is what we'll try in the exercises.\n",
    "\n",
    "# Conclusion #\n",
    "\n",
    "In this tutorial, you saw how to build a custom convnet composed of many **convolutional blocks** and capable of complex feature engineering. \n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "In the exercises, you'll create a convnet that performs as well on this problem as VGG16 does -- without pretraining! [**Try it now!**](#$NEXT_NOTEBOOK_URL$)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
