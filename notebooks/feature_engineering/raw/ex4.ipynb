{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In this exercise you'll use some feature selection algorithms to improve your model. Some of these methods take a while to run so I'll have you write functions which I'll test on small samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering.ex4 import *\n",
    "\n",
    "import os\n",
    "\n",
    "clicks = pd.read_parquet('../input/feature-engineering-data/baseline_data.pqt')\n",
    "data_files = ['count_encodings.pqt',\n",
    "              'catboost_encodings.pqt',\n",
    "              'interactions.pqt',\n",
    "              'past_6hr_events.pqt',\n",
    "              'downloads.pqt',\n",
    "              'time_deltas.pqt',\n",
    "              'svd_encodings.pqt']\n",
    "data_root = '../input/feature-engineering-data'\n",
    "for file in data_files:\n",
    "    features = pd.read_parquet(os.path.join(data_root, file))\n",
    "    clicks = clicks.join(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Score\n",
    "\n",
    "Let's look at the baseline score for all the features we've made so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "_, baseline_score, _ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Which data to use for feature selection?\n",
    "\n",
    "Since many feature selection methods require calculating statistics from the dataset, should you use all the data for feature selection?\n",
    "\n",
    "Uncomment the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 91 features we're using for predictions. With all these features, there is a good chance the model is overfitting the data. We might be able to reduce the overfitting by removing some features. Of course, the model's performance might decrease. But at least we'd be making the model smaller and faster without losing much performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Univariate Feature Selection\n",
    "\n",
    "Below, use `SelectKBest` with the `f_classif` scoring function to choose 40 features from the 91 features in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the selector, keeping 40 features\n",
    "selector = ____\n",
    "\n",
    "# Use the selector to retrieve the best features\n",
    "X_new = ____ \n",
    "\n",
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = ____\n",
    "\n",
    "# Find the columns that were dropped\n",
    "dropped_columns = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if you need some guidance\n",
    "q_2.hint()\n",
    "q_2.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Do feature extraction on the training data only!\n",
    "selector = SelectKBest(f_classif, k=40)\n",
    "X_new = selector.fit_transform(train[feature_cols], train['is_attributed'])\n",
    "\n",
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "\n",
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]\n",
    "\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1),\n",
    "                test.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) The best value of K\n",
    "\n",
    "With this method we can choose the best K features, but we still have to choose K ourselves. How would you find the \"best\" value of K? That is, you want it to be small so you're keeping the best features, but not so small that it's degrading the model's performance.\n",
    "\n",
    "Uncomment the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Use L1 regularization for feature selection\n",
    "\n",
    "Now try a more powerful approach using L1 regularization. Implement a function `select_features_l1` that returns a list of features to keep.\n",
    "\n",
    "Use a `LogisticRegression` classifier model with an L1 penalty to select the features. For the model, set the random state to 7 and the regularization parameter to 0.1. Fit the model then use `SelectFromModel` to return a model with the selected features.\n",
    "\n",
    "The checking code will run your function on a sample from the dataset to provide more immediate feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "    \"\"\" Return selected features using logistic regression with an L1 penalty \"\"\"\n",
    "    ____\n",
    "\n",
    "    return ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you're feeling stuck\n",
    "#q_4.hint()\n",
    "#q_4.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your work\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "    logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n",
    "    model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "    X_new = model.transform(X)\n",
    "    \n",
    "    # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "    selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                     index=X.index,\n",
    "                                     columns=X.columns)\n",
    "    \n",
    "    # Dropped columns have values of all 0s, keep other columns \n",
    "    cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "    \n",
    "    return cols_to_keep\n",
    "\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "X, y = train[feature_cols][:10000], train['is_attributed'][:10000]\n",
    "\n",
    "selected = select_features_l1(X, y)\n",
    "\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = feature_cols.drop(selected)\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1),\n",
    "                test.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Selection with Trees\n",
    "\n",
    "Since we're using a tree-based model, using another tree-based model for feature selection might produce better results. What would you do different to select the features using a trees classifier?\n",
    "\n",
    "Uncomment the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_5.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Principle Component Analysis\n",
    "\n",
    "There are 63 numerical features, the SVD encodings plus the three numerical features we created. Use PCA to reduce these to down to 20 features and refit the model. In this first step here, create the PCA transformer and fit it to your data. To make it reproduceable (and so we can check your code) set the random state to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "feature_cols = train.columns[-63:]\n",
    "\n",
    "# Create the PCA transformer with 20 components\n",
    "pca = ____\n",
    "\n",
    "# Fit PCA to the feature columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you're feeling stuck\n",
    "#q_6.hint()\n",
    "#q_6.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your work\n",
    "q_6.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "feature_cols = train.columns[-63:]\n",
    "\n",
    "pca = PCA(n_components=20, random_state=7)\n",
    "pca.fit(train[feature_cols], train['is_attributed'])\n",
    "q_6.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Applying PCA encodings\n",
    "\n",
    "Implement a function `encode_pcs` that encodes the feature columns of a dataframe using a trained PCA transformer. As input, this function will take a dataframe, a trained PCA transformer, and a list of feature columns to encode. Then it should return a dataframe with the same index, but encoded features. Note that the feature columns here should be the same as what was used to train the PCA transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pcs(df, pca, feature_cols):\n",
    "    \"\"\" Returns a new dataframe with the feature columns of a dataframe (defined with the\n",
    "        feature_cols argument) encoded using a trained PCA transformer\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        df: DataFrame\n",
    "        pca: Trained PCA transformer\n",
    "        feature_cols: the feature columns of df that will be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame with PCA encoded features\n",
    "    \"\"\"\n",
    "    ____\n",
    "    return ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you're feeling stuck\n",
    "# q_7.hint()\n",
    "# q_7.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "def encode_pcs(df, pca, feature_cols):\n",
    "    encodings = pd.DataFrame(pca.transform(df[feature_cols]),\n",
    "                             index=df.index).add_prefix('pca_')\n",
    "    encoded_df = df.drop(feature_cols, axis=1).join(encodings)\n",
    "    return encoded_df\n",
    "\n",
    "q_7.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_model(encode_pcs(train, pca, feature_cols),\n",
    "                encode_pcs(valid, pca, feature_cols), \n",
    "                encode_pcs(test, pca, feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Feature Selection with Boruta\n",
    "\n",
    "Finally, you'll use Boruta. Define a function `fit_boruta` that fits the Boruta feature selection to a dataset and returns the selected features. This function will take a dataframe with the data, a list of feature columns, and the target column (as a string). When creating the Boruta selector, set the random state to 7.\n",
    "\n",
    "Boruta takes a while to run so to provide more immediate feedback I'll only be testing it on a small sample from the data. Typically, you'd let it run on the entire dataset and just wait. I'll provide a file with the results on the full dataset so you can see the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_boruta(df, feature_cols, target):\n",
    "    \"\"\" Returns a new dataframe with the feature columns of a dataframe (defined with the\n",
    "        feature_cols argument) encoded using Boruta\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        df: input DataFrame\n",
    "        feature_cols: the feature columns of df that will be encoded\n",
    "        target: the target column in df\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List (or list-like) of the selected features\n",
    "    \"\"\"\n",
    "    # Set random state to 7\n",
    "    ____\n",
    "    return ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you're feeling stuck\n",
    "#q_8.hint()\n",
    "#q_8.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to check your work\n",
    "q_8.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_boruta(df, feature_cols, target):\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    # define random forest classifier, with utilising all cores and\n",
    "    # sampling in proportion to y labels\n",
    "    rf = RandomForestClassifier(class_weight='balanced', max_depth=5, \n",
    "                                n_jobs=-1, random_state=7)\n",
    "\n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(rf, n_estimators='auto', random_state=7)\n",
    "\n",
    "    # Fit the Boruta selector\n",
    "    feat_selector.fit(X, y)\n",
    "\n",
    "    # Get the selected columns\n",
    "    selected_columns = feature_cols[feat_selector.support_]\n",
    "    return selected_columns\n",
    "\n",
    "q_8.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Still need to fit boruta on the entire dataset and save the results.\n",
    "## I'll load in the smaller dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "selected = fit_boruta(train[:5000], feature_cols, 'is_attributed')\n",
    "rejected_columns = feature_cols.drop(selected)\n",
    "\n",
    "_ = train_model(train.drop(rejected_columns, axis=1),\n",
    "                valid.drop(rejected_columns, axis=1),\n",
    "                test.drop(rejected_columns, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
