{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In the exercise, you will work with data from the TalkingData AdTracking competition.  The goal of the competition is to predict if a user will download an app after clicking through an ad. \n",
    "\n",
    "<center><a href=\"https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection\"><img src=\"https://storage.googleapis.com/kaggle-media/learn/images/srKxEkD.png\" width=600px></a></center>\n",
    "\n",
    "For this course you will use a small sample of the data, dropping 99% of negative records (where the app wasn't downloaded) to make the target more balanced.\n",
    "\n",
    "After building a baseline model, you'll be able to see how your feature engineering and selection efforts improve the model's performance.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Begin by running the code cell below to set up the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering.ex1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "The first thing you'll do is construct a baseline model. We'll begin by looking at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "click_data = pd.read_csv('../input/feature-engineering-data/train_sample.csv',\n",
    "                         parse_dates=['click_time'])\n",
    "click_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Construct features from timestamps\n",
    "\n",
    "Notice that the `click_data` DataFrame has a `'click_time'` column with timestamp data.\n",
    "\n",
    "Use this column to create features for the coresponding day, hour, minute and second. \n",
    "\n",
    "Store these as new integer columns `day`, `hour`, `minute`, and `second` in a new DataFrame `clicks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for timestamp features day, hour, minute, and second\n",
    "clicks = click_data.copy()\n",
    "clicks['day'] = clicks['click_time'].dt.day.astype('uint8')\n",
    "# Fill in the rest\n",
    "clicks['hour'] = ____\n",
    "clicks['minute'] = ____\n",
    "clicks['second'] = ____\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you need guidance\n",
    "q_1.hint()\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# My solution, there are a lot of ways to do this\n",
    "clicks = click_data.copy()\n",
    "\n",
    "# Split up the times\n",
    "click_times = click_data['click_time']\n",
    "clicks['day'] = click_times.dt.day.astype('uint8')\n",
    "# Fill in the rest\n",
    "clicks['hour'] = click_times.dt.hour.astype('uint8')\n",
    "clicks['minute'] = click_times.dt.minute.astype('uint8')\n",
    "clicks['second'] = click_times.dt.second.astype('uint8')\n",
    "q_1.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Label Encoding\n",
    "For each of the categorical features `['ip', 'app', 'device', 'os', 'channel']`, use scikit-learn's `LabelEncoder` to create new features in the `clicks` DataFrame. The new column names should be the original column name with `'_labels'` appended, like `ip_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "\n",
    "# Create new columns in clicks using preprocessing.LabelEncoder()\n",
    "for feature in cat_features:\n",
    "    ____\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these if you need guidance\n",
    "# q_2.hint()\n",
    "# q_2.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Label encode categorical variables \n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "for feature in cat_features:\n",
    "    new_feature = label_encoder.fit_transform(clicks[feature])\n",
    "    clicks[feature +'_labels'] = new_feature\n",
    "    \n",
    "q_2.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "clicks.drop(['ip', 'app', 'device', 'os', 'channel'], axis=1, inplace=True)\n",
    "    \n",
    "q_2.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to view your new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) One-hot Encoding\n",
    "\n",
    "In the code cell above, you used label encoded features.  Would it have also made sense to instead use one-hot encoding for the categorical variables `'ip'`, `'app'`, `'device'`, `'os'`, or `'channel'`?\n",
    "\n",
    "**Note**: If you're not familiar with one-hot encoding, please check out **[this lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** from the Intermediate Machine Learning course.\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation, and test sets\n",
    "With our baseline features ready, we need to split our data into training and validation sets. We should also hold out a test set to measure the final accuracy of the model.\n",
    "\n",
    "### 4) Train/test splits with time series data\n",
    "This is time series data. Are there any special considerations when creating train/test splits for time series? If so, what are they?\n",
    "\n",
    "Uncomment the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_4.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/validation/test splits\n",
    "\n",
    "Here we'll create training, validation, and test splits. First, `clicks` DataFrame is sorted in order of increasing time. The first 80% of the rows are the train set, the next 10% are the validation set, and the last 10% are the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['day', 'hour', 'minute', 'second', \n",
    "                'ip_labels', 'app_labels', 'device_labels',\n",
    "                'os_labels', 'channel_labels']\n",
    "\n",
    "valid_fraction = 0.1\n",
    "clicks_srt = clicks.sort_values('click_time')\n",
    "valid_rows = int(len(clicks_srt) * valid_fraction)\n",
    "train = clicks_srt[:-valid_rows * 2]\n",
    "# valid size == test size, last two sections of the data\n",
    "valid = clicks_srt[-valid_rows * 2:-valid_rows]\n",
    "test = clicks_srt[-valid_rows:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LightGBM\n",
    "\n",
    "Now we can create LightGBM dataset objects for each of the smaller datasets and train the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Finally, with the model trained, we evaluate its performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['is_attributed'], ypred)\n",
    "print(f\"Test score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our baseline score for the model. When we transform features, add new ones, or perform feature selection, we should be improving on this score. However, since this is the test set, we only want to look at it at the end of all our manipulations. At the very end of this course you'll look at the test score again to see if you improved on the baseline model.\n",
    "\n",
    "# Keep Going\n",
    "Now that you have a baseline model, you are ready to **[use categorical encoding techniques](#$NEXT_NOTEBOOK_URL$)** to improve it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
