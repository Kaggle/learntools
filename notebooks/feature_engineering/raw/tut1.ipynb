{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this micro-course, you will learn a practical approach to feature engineering. You'll be able to apply what you learn to Kaggle competitions as well as building machine learning applications. Through hands-on exercises you will:\n",
    "\n",
    "* Develop a baseline model for comparing performance on models with more features\n",
    "* Encode categorical features so the model can make better use of the information\n",
    "* Generate new features to provide more information for the model\n",
    "* Select features to reduce overfitting and increase prediction speed\n",
    "\n",
    "In the exercise, you will apply feature engineering techniques to data from the TalkingData AdTracking competition.\n",
    "\n",
    "<center><a href=\"https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection\"><img src=\"https://i.imgur.com/srKxEkD.png\" width=600px></a></center>\n",
    "\n",
    "The goal of the competition is to predict if a user will download an app after clicking through an ad. For this course you will use a small sample of the data, dropping 99% of negative records (where the app wasn't downloaded) to make the target more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kickstarter projects\n",
    "\n",
    "To help you apply the techniques, you'll first see them applied using data from Kickstarter projects. The first few rows of the Kickstarter projects data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n",
    "                 parse_dates=['deadline', 'launched'])\n",
    "ks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do here is predict if a Kickstarter project will succeed. We get the outcome from the `state` column. To predict the outcome we can use features such as category, currency, funding goal, country, and when it was launched. \n",
    "\n",
    "### Preparing target column\n",
    "\n",
    "First I'll look at project states and convert the column into something we can use as targets in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(ks.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have six states, how many records of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.groupby('state')['ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning isn't the current focus, so we'll simplify this example by:\n",
    "\n",
    "- Dropping projects that are \"live\"\n",
    "- Counting \"successful\" states as `outcome = 1`\n",
    "- Combining every other state as `outcome = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting timestamps\n",
    "\n",
    "I convert the `launched` feature into categorical features we can use in a model. Since I loaded in the columns as timestamp data, I access date and time values through the `.dt` attribute on the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "ks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping categorical variables\n",
    "\n",
    "Now for the categorical variables -- `category`, `currency`, and `country` -- I'll need to convert them into integers so our model can use the data. For this I'll use scikit-learn's `LabelEncoder`. This assigns an integer to each value of the categorical feature and replaces those values with the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder to each column\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll collect all the features we'll use in a new dataframe and use that to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since ks and encoded have the same index and I can easily join them\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training, validation, and test splits\n",
    "\n",
    "We need to create data sets for training, validation, and testing. We'll use a fairly simple approach and split the data using slices. We'll use 10% of the data as a validation set, 10% for testing, and the other 80% for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "test = data[-valid_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you want to be careful that each data set has the same proportion of target classes. I'll print out the fraction of successful outcomes for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction = {each.outcome.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, each set is around 35% true outcomes likely because the data was well randomized beforehand. A good way to do this automatically is with `sklearn.model_selection.StratifiedShuffleSplit` but I don't need to use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a LightGBM model\n",
    "\n",
    "For this course we'll be using a LightGBM model. This is a tree-based model that typically provides the best performance, even compared to XGBoost. It's also relatively fast to train. We won't do hyperparameter optimization because that isn't the goal of this course. So, our models won't be the absolute best performance you can get. But you'll still see model performance improve as we do feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions & evaluating the model\n",
    "\n",
    "Finally, let's make predictions on the test set with the model and see how well it performs. An important thing to remember is that you can overfit to the validation data. This is why we need a test set that the model never sees until the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "Now you'll build your own **[baseline model](#$NEXT_NOTEBOOK_URL$)** which you can improve with feature engineering techniques as you go through the course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
