{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependence Plots\n",
    "\n",
    "While feature importance shows what variables most affect predictions, partial dependence plots show *how* a feature affects predictions.\n",
    "\n",
    "This is useful to answer questions like:\n",
    "\n",
    "* Controlling for all other house features, what impact do longitude and latitude have on home prices? To restate this, how would similarly sized houses be priced in different areas?\n",
    "\n",
    "* Are predicted health differences between two groups due to differences in their diets, or due to some other factor?\n",
    "\n",
    "If you are familiar with linear or logistic regression models, partial dependence plots can be interpreted similarly to the coefficients in those models.  Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models.  If you aren't familiar with linear or logistic regressions, don't worry about this comparison.\n",
    "\n",
    "We will show a couple examples, explain the interpretation of these plots, and then review the code to create these plots.\n",
    "\n",
    "# How it Works\n",
    "\n",
    "Like permutation importance, **partial dependence plots are calculated after a model has been fit.**  The model is fit on real data that has not been artificially manipulated in any way.  \n",
    "\n",
    "In our soccer example, teams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features.\n",
    "\n",
    "To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\n",
    "\n",
    "We will use the fitted model to predict our outcome (probability their player won \"man of the match\"). But we **repeatedly alter the value for one variable** to make a series of predictions.  We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time.  Then predict again for 60%.  And so on.  We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n",
    "\n",
    "In this description, we used only a single row of data.  Interactions between features may cause the plot for a single row to be atypical.  So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.  \n",
    "\n",
    "# Code Example\n",
    "\n",
    "Model building isn't our focus, so we won't focus on the data exploration or model building code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\n",
    "y = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\n",
    "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
    "X = data[feature_names]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first example uses a decision tree, which you can see below. In practice, you'll use more sophistated models for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As guidance to read the tree:\n",
    "- Leaves with children show their splitting criterion on the top\n",
    "- The pair of values at the bottom show the count of False values and True values for the target respectively, of data points in that node of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to create the Partial Dependence Plot using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Create and plot the data\n",
    "disp1 = PartialDependenceDisplay.from_estimator(tree_model, val_X, ['Goal Scored'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n",
    "\n",
    "From this particular graph, we see that scoring a goal substantially increases your chances of winning \"Man of The Match.\"  But extra goals beyond that appear to have little impact on predictions.\n",
    "\n",
    "Here is another example plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot = 'Distance Covered (Kms)'\n",
    "disp2 = PartialDependenceDisplay.from_estimator(tree_model, val_X, [feature_to_plot])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure.\n",
    "\n",
    "You can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "disp3 = PartialDependenceDisplay.from_estimator(rf_model, val_X, [feature_to_plot])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model thinks you are more likely to win *Man of the Match* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions.\n",
    "\n",
    "In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n",
    "\n",
    "# 2D Partial Dependence Plots\n",
    "If you are curious about interactions between features, 2D partial dependence plots are also useful. An example may clarify this.  \n",
    "\n",
    "We will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "f_names = [('Goal Scored', 'Distance Covered (Kms)')]\n",
    "# Similar to previous PDP plot except we use tuple of features instead of single feature\n",
    "disp4 = PartialDependenceDisplay.from_estimator(tree_model, val_X, f_names, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows predictions for any combination of Goals Scored and Distance covered. \n",
    "\n",
    "For example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.  If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals?\n",
    "\n",
    "But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "**[Test your understanding](#$NEXT_NOTEBOOK_URL$)** on conceptual questions and a short coding challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
