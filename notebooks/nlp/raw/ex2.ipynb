{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Classification\n",
    "\n",
    "In this exercise, you will train a model that classifies Yelp reviews into \"good\" or \"bad\" sentiments. You'll use ScaPy to train a text classifier, then use the model to predict the sentiment of text examples.\n",
    "\n",
    "The data consists of the text body of each review along with the star rating. The star ratings have been grouped into sentiments. Ratings with 1-2 stars are \"negative\", ratings with 4-5 stars are \"positive\", while 3 star ratings are \"neutral\" and have been dropped from the data.\n",
    "\n",
    "<img src=\"https://i.imgur.com/7l6vwIr.png\" width=400px>\n",
    "\n",
    "The goal then is to use the text and sentiments of each review to train a classification model for predicting the sentiment of new text. To do this, you'll use ScaPy's TextCategorizer component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.nlp.ex2 import *\n",
    "print(\"\\nSetup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Exercise: Create the model\n",
    "\n",
    "For the first exercise, create the text classifier model and add the labels `\"NEGATIVE\"` and `\"POSITIVE\"`. For the model, use the `\"bow\"` (bag of words) architecture. The other architectures will likely result in better performance, but train much slower. Also, configure the model to use exclusive classes since each review can only be positive or negative, exclusive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = ____\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = ____\n",
    "\n",
    "# Add NEGATIVE and POSITIVE labels to text classifier\n",
    "____\n",
    "\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need some guidance\n",
    "# q_1.hint()\n",
    "# q_1.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Create an empty model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
    "textcat = nlp.create_pipe(\n",
    "            \"textcat\",\n",
    "            config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"bow\"})\n",
    "nlp.add_pipe(textcat)\n",
    "\n",
    "# Add NEGATIVE and POSITIVE labels to text classifier\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "\n",
    "q_1.assert_check_passed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "Here I've included a function to load the data and split it into training and validation slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_file, split=0.9):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Shuffle data\n",
    "    train_data = data.sample(frac=1, random_state=7)\n",
    "    \n",
    "    texts = train_data.text.values\n",
    "    labels = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)}\n",
    "              for y in train_data.sentiment.values]\n",
    "    split = int(len(train_data) * split)\n",
    "    \n",
    "    train_labels = [{\"cats\": labels} for labels in labels[:split]]\n",
    "    val_labels = [{\"cats\": labels} for labels in labels[split:]]\n",
    "    \n",
    "    return texts[:split], train_labels, texts[split:], val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, val_texts, val_labels = load_data('../input/nlp-course/yelp_ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Exercise: Train Function\n",
    "\n",
    "Implement a function `train` that updates a model with training data. First you'll need to shuffle the training data. Then, create batches with `minibatch`. Since `model.update` expects separate lists for texts and labels, you'll need to split the batches into these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "def train(model, train_data, optimizer):\n",
    "    losses = {}\n",
    "    random.seed(1)\n",
    "    # Shuffle the training data\n",
    "    ____\n",
    "    \n",
    "    # Create batches with batch size = 8\n",
    "    batches = \"____\"\n",
    "    for batch in batches:\n",
    "        # train_data is a list of tuples [(text0, label0), (text1, label1), ...]\n",
    "        # Split batch into texts and labels\n",
    "        ____\n",
    "        \n",
    "        # Update model with texts and labels\n",
    "        ____\n",
    "        \n",
    "    return losses\n",
    "\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need some guidance\n",
    "# q_2.hint()\n",
    "# q_2.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "def train(model, train_data, optimizer, batch_size=8):\n",
    "    losses = {}\n",
    "    #random.seed(1)\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    for batch in batches:\n",
    "        texts, labels = zip(*batch)\n",
    "        model.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    return losses\n",
    "\n",
    "q_2.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for reproducibility\n",
    "spacy.util.fix_random_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "losses = train(nlp, train_data, optimizer)\n",
    "print(losses['textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this slightly trained model on some example text and look at the probabilities assigned to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This tea cup was full of holes. Do not recommend.\"\n",
    "doc = nlp(text)\n",
    "print(doc.cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities look reasonable. Now you should turn them into an actual prediction.\n",
    "\n",
    "## 3) Exercise: Making Predictions\n",
    "\n",
    "Implement a function `predict` that uses a model to predict the sentiment of text examples. The function takes a SpaCy model (with a TextCategorizer) and a list of texts. First, tokenize the texts using `model.tokenizer`. Then, pass those docs to the TextCategorizer which you can get from `model.get_pipe`. Use the `textcat.predict` method to get scores for each document, then choose the class with the highest score (probability) as the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts): \n",
    "    # Use the model's tokenizer to tokenize each input text\n",
    "    docs = ____\n",
    "    \n",
    "    # Use textcat to get the scores for each doc\n",
    "    ____\n",
    "    \n",
    "    # From the scores, find the class with the highest score/probability\n",
    "    predicted_class = ____\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "q_3.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need some guidance\n",
    "# q_3.hint()\n",
    "# q_3.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "def predict(model, texts): \n",
    "    # Use the tokenizer to tokenize each input text example\n",
    "    docs = [model.tokenizer(text) for text in texts]\n",
    "    \n",
    "    # Use textcat to get the scores for each doc\n",
    "    textcat = model.get_pipe('textcat')\n",
    "    scores, _ = textcat.predict(docs)\n",
    "    \n",
    "    # From the scores, find the class with the highest score/probability\n",
    "    predicted_class = scores.argmax(axis=1)\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "q_3.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = val_texts[34:38]\n",
    "predictions = predict(nlp, texts)\n",
    "\n",
    "for p, t in zip(predictions, texts):\n",
    "    print(f\"{textcat.labels[p]}: {t} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye it looks like your model is working well after going through the data just once. However you need to calculate some metric for the model's performance on the hold-out validation data.\n",
    "\n",
    "## 4) Exercise: Evaluating a Trained Model\n",
    "\n",
    "Implement a function that evaluates a `TextCategorizer` model. This function `evaluate` takes a model along with texts and labels. It returns the accuracy of the model, the number of correct predictions divided by all predictions.\n",
    "\n",
    "First, use the `predict` method you wrote earlier to get the predicted class for each text in `texts`. Then, find where the predicted labels match the true \"gold-standard\" labels and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, texts, labels):\n",
    "    \"\"\" Returns the accuracy of a TextCategorizer model. \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        model: ScaPy model with a TextCategorizer\n",
    "        texts: Text samples, from load_data function\n",
    "        labels: True labels, from load_data function\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get predictions from textcat model (using your predict method)\n",
    "    predicted_class = ____\n",
    "    \n",
    "    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)\n",
    "    true_class = ____\n",
    "    \n",
    "    # A boolean or int array indicating correct predictions\n",
    "    correct_predictions = ____\n",
    "    \n",
    "    # The accuracy, number of correct predictions divided by all predictions\n",
    "    accuracy = ____\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need some guidance\n",
    "# q_4.hint()\n",
    "# q_4.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "\n",
    "def evaluate(model, texts, labels):\n",
    "    \"\"\" Returns the accuracy of a TextCategorizer model. \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        model: ScaPy model with a TextCategorizer\n",
    "        texts: Text samples, from load_data function\n",
    "        labels: True labels, from load_data function\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get predictions from textcat model\n",
    "    predicted_class = predict(model, texts)\n",
    "    \n",
    "    # From labels, get the true class as a list of integers (POSITIVE -> 1, NEGATIVE -> 0)\n",
    "    true_class = [int(each['cats']['POSITIVE']) for each in labels]\n",
    "    \n",
    "    # A boolean or int array indicating correct predictions\n",
    "    correct_predictions = predicted_class == true_class\n",
    "    \n",
    "    # The accuracy, number of correct predictions divided by all predictions\n",
    "    accuracy = correct_predictions.mean()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "q_4.assert_check_passed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the functions implemented, you can train and evaluate in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 5\n",
    "for i in range(n_iters):\n",
    "    losses = train(nlp, train_data, optimizer)\n",
    "    accuracy = evaluate(nlp, val_texts, val_labels)\n",
    "    print(f\"Loss: {losses['textcat']:.3f} \\t Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) What would you do to find the best model?\n",
    "\n",
    "In this exercise, you only build the necessary components to train a text classifier with SpaCy. What could you do further to optimize the model and get the best accuracy on the hold-out data?\n",
    "\n",
    "Uncomment the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_5.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up!\n",
    "\n",
    "In the next lesson, you'll learn how to use SpaCy to represent tokens as vectors, then use these vectors to train machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
