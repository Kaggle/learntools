{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Data comes in many different forms such as time stamps, sensor readings, images, categorical labels, and much much more. A large amount of data exists as language, in text and speech.  The field of using computers to understand language data is known as Natural Language Processing (NLP).\n",
    "\n",
    "This understanding can come in the form of information extraction for observing trends. For example, Google can scan billions of searches to track how specific terms change in frequency over time.\n",
    "\n",
    "![Google trends for SpaCy, NLTK, and Gensim libraries](https://i.imgur.com/QR7eIjt.png)\n",
    "\n",
    "Or consider that some term is showing up in a lot of customer support tickets. You can have a program observe these tickets for frequent terms and alert the appropriate product team. \n",
    "\n",
    "Within machine learning, and in this course, we're more interested in using language data to build predictive models. As with other domains, much of the work in NLP is finding ways to represent text or speech such that it can be used with machine learning models. That is, we need to convert documents or words or even individual characters into numbers and vectors. These vectors can then be used as input to models.\n",
    "\n",
    "As with other domains, you can break down NLP into supervised and unsupervised tasks. In this course, you'll be implementing supervised text classification models.\n",
    "\n",
    "## Outline\n",
    "\n",
    "In this course we'll show you how to use SpaCy for NLP in tutorials, then you'll implement the code yourself in exercises. You'll learn about:\n",
    "\n",
    "* Text processing and pattern matching\n",
    "* Text classification models with SpaCy\n",
    "* Word vectors & embeddings used with scikit-learn models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note before you get started\n",
    "\n",
    "This mini-course was built assuming you already have some experience with machine learning. If you don't have experience with supervised learning and the scikit-learn library, please take the Intro to Machine Learning and Intermediate Machine Learning mini-courses before continuing on with these tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into this dataset https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with SpaCy\n",
    "\n",
    "In this course you'll be using the spaCy library to extract information from text and to convert text into vectors for classification models. SpaCy is relatively new and has quickly become the most popular Python frameworks. Personally, I find it to be intuitive to use and backed up by excellent documentation.\n",
    "\n",
    "To use spaCy, you need to load a **model**. Models are language specific and come in different sizes, typically small, medium, and large. Larger models have more capabilities but also consume more memory, run slower, and take longer to load.\n",
    "\n",
    "To use a spaCy model, you load it with `spacy.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code loads the small English model. When loading models, you might run into an error like this:\n",
    "```\n",
    "OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
    "```\n",
    "This error means the model doesn't exist on your machine. You'll need to download the model with SpaCy by running\n",
    "\n",
    "```\n",
    "python -m spacy download model_name\n",
    "```\n",
    "\n",
    "in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model loaded, we can use it to process some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy, calming, and delicious, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n",
    "\n",
    "This returns a document object that contains **tokens**. A token is one unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". To get the tokens you iterate through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are `token.lemma_` and `token.is_stop`.\n",
    "\n",
    "# Text preprocessing\n",
    "\n",
    "The \"lemma\" of a word is its base form. For example, \"to be\" is the root verb of \"is\". The lemma of \"is\" then, is \"be\". Removing prefixes and suffixes also results in lemmas, such as changing \"calming\" to \"calm\". Converting words in text to their lemma version is often called \"lemmatizing\" or \"normalization\".\n",
    "\n",
    "Stopwords are words that occur frequently in the language and don't contain much information. In English, stopwords include \"the\", \"is\", \"and\", \"but\", \"not\". With a spaCy token, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<15}{:<15}{}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(\"{:<15}{:<15}{}\".format(str(token), token.lemma_, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are lemmas and identifying stopwords important? Language data tends to have a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy, calming, and delicious. Removing the stop words might improve the quality of the data for use in predictive models. Using lemma forms helps reduce noise as well by reducing multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "However, lemmatizing and dropping stopwords might result in your models performing worse. You'll need to treat this preprocessing as part of your hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Matching\n",
    "\n",
    "Another common use of spaCy is matching tokens or phrases within chunks of text or whole documents. Pattern matching is often done with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a `Matcher`. When you want to match a list of terms, it's easier and more efficient to use `PhraseMatcher`. For example, if you want to find where different smartphone models show up in some text you can create patterns for the model names of interest. First you create the `PhraseMatcher` itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting `attr='LOWER'` will match the phrases on lowercased text. This provides case insensitive matching.\n",
    "\n",
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the `nlp` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you create a document from the text to search and use the phrase matcher to find where the termcs occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side photography \"\n",
    "               \"tests pitting the iPhone 11 Pro against the Galaxy Note 10 Plus and last yearâ€™s \" \n",
    "               \"iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen a few uses of SpaCy for NLP, it's your turn to try it. In the next exercise, you'll use the `PhraseMatcher` to perform some analysis on Yelp reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
