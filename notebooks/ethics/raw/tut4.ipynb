{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004205,
     "end_time": "2021-02-17T20:43:06.853100",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.848895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "There are many different ways of defining what we might look for in a fair machine learning (ML) model.  For instance, say we're working with a model that approves (or denies) credit card applications.  Is it:\n",
    "- fair if the approval rate is equal across genders, or is it \n",
    "- better if gender is removed from the dataset and hidden from the model?\n",
    "\n",
    "In this tutorial, we'll cover four criteria that we can use to decide if a model is fair.  Then, you'll apply what you've learned in a **[hands-on exercise](#$NEXT_NOTEBOOK_URL$)** where you'll run code to train several models and analyze fairness of each.  (*As for every lesson in this course, no prior coding experience is required.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003012,
     "end_time": "2021-02-17T20:43:06.859709",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.856697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Four fairness criteria\n",
    "\n",
    "These four fairness criteria are a useful starting point, but it's important to note that there are more ways of formalizing fairness, which you are encouraged to [explore](https://arxiv.org/pdf/1710.03184.pdf).  \n",
    "\n",
    "Assume we're working with a model that selects individuals to receive some outcome.  For instance, the model could select people who should be approved for a loan, accepted to a university, or offered a job opportunity.  (*So, we don't consider models that perform tasks like facial recognition or text generation, among other things.*)\n",
    "\n",
    "## 1. Demographic parity / statistical parity\n",
    "\n",
    "**Demographic parity** says the model is fair if the composition of people who are selected by the model matches the group membership percentages of the applicants.  \n",
    "> A nonprofit is organizing an international conference, and 20,000 people have signed up to attend.  The organizers write a ML model to select 100 attendees who could potentially give interesting talks at the conference.  Since 50% of the attendees will be women (10,000 out of 20,000), they design the model so that 50% of the selected speaker candidates are women.\n",
    "\n",
    "## 2. Equal opportunity\n",
    "\n",
    "**Equal opportunity** fairness ensures that the proportion of people who should be selected by the model (\"positives\") that are correctly selected by the model is the same for each group.   We refer to this proportion as the **true positive rate** (TPR) or **sensitivity** of the model.\n",
    "\n",
    "> A doctor uses a tool to identify patients in need of extra care, who could be at risk for developing serious medical conditions.  (This tool is used only to supplement the doctor's practice, as a second opinion.)  It is designed to have a high TPR that is equal for each demographic group.    \n",
    "\n",
    "## 3. Equal accuracy\n",
    "\n",
    "Alternatively, we could check that the model has **equal accuracy** for each group.  That is, the percentage of correct classifications (people who should be denied and are denied, and people who should be approved who are approved) should be the same for each group.  If the model is 98% accurate for individuals in one group, it should be 98% accurate for other groups.  \n",
    "\n",
    "> A bank uses a model to approve people for a loan.  The model is designed to be equally accurate for each demographic group: this way, the bank avoids approving people who should be rejected (which would be financially damaging for both the applicant and the bank) and avoid rejecting people who should be approved (which would be a failed opportunity for the applicant and reduce the bank's revenue). \n",
    "\n",
    "## 4. Group unaware / \"Fairness through unawareness\"\n",
    "\n",
    "**Group unaware** fairness removes all group membership information from the dataset.  For instance, we can remove gender data to try to make the model fair to different gender groups.  Similarly, we can remove information about race or age.\n",
    "\n",
    "> One difficulty of applying this approach in practice is that one has to be careful to identify and remove proxies for the group membership data.  For instance, in cities that are racially segregated, zip code is a strong proxy for race.  That is, when the race data is removed, the zip code data should also be removed, or else the ML application may still be able to infer an individual's race from the data. Additionally, group unaware fairness is unlikely to be a good solution for historical bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002805,
     "end_time": "2021-02-17T20:43:06.865652",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.862847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example\n",
    "\n",
    "We'll work with a small example to illustrate the differences between the four different types of fairness.  We'll use a **confusion matrix**, which is a common tool used to understand the performance of a ML model.  This tool is depicted in the example below, which depicts a model with 80% accuracy (since 8/10 people were correctly classified) and has an 83% true positive rate (since 5/6 \"positives\" were correctly classified).\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/xFZG5fF.png)\n",
    "\n",
    "To understand how a model's performance varies across groups, we can construct a different confusion matrix for each group. In this small example, we'll assume that we have data from only 20 people, equally split between two groups (10 from Group A, and 10 from Group B).  \n",
    "\n",
    "The next image shows what the confusion matrices could look like, if the model satisfies **demographic parity** fairness. 10 people from each group (50% from Group A, and 50% from Group B) were considered by the model. 14 people, also equally split across groups (50% from Group A, and 50% from Group B) were approved by the model. \n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/e32gcDh.png)\n",
    "\n",
    "For ****equal opportunity**** fairness, the TPR for each group should be the same; in the example below, it is 66% in each case.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/aInWboA.png)\n",
    "\n",
    "Next, we can see how the confusion matrices might look for ****equal accuracy**** fairness. For each group, the model was 80% accurate.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/fIOJovc.png)\n",
    "\n",
    "Note that ****group unaware**** fairness cannot be detected from the confusion matrix, and is more concerned with removing group membership information from the dataset.\n",
    "\n",
    "Take the time now to study these toy examples, and use it to build your intuition for the differences between the different types of fairness. How does the example change if Group A has double the number of applicants of Group B?\n",
    "\n",
    "Also note that none of the examples satisfy more than one type of fairness. For instance, the demographic parity example does not satisfy equal accuracy or equal opportunity. Take the time to verify this now. In practice, it is not possible to optimize a model for more than one type of fairness: to read more about this, explore the [Impossibility Theorem of Machine Fairness](https://arxiv.org/abs/2007.06024).  _So which fairness criterion should you select, if you can only satisfy one?_  As with most ethical questions, the correct answer is usually not straightforward, and picking a criterion should be a long conversation involving everyone on your team.\n",
    "\n",
    "When working with a real project, the data will be much, much larger.  In this case, confusion matrices are still a useful tool for analyzing model performance.  One important thing to note, however, is that real-world models typically cannot be expected to satisfy any fairness definition perfectly.  For instance, if \"demographic parity\" is chosen as the fairness metric, where the goal is for a model to select 50% men, it may be the case that the final model ultimately selects some percentage close to, but not exactly 50% (like 48% or 53%). \n",
    "\n",
    "\n",
    "# Learn more\n",
    "\n",
    "- Explore different types of fairness with an [interactive tool](http://research.google.com/bigpicture/attacking-discrimination-in-ml/).\n",
    "- You can read more about equal opportunity in [this blog post](https://ai.googleblog.com/2016/10/equality-of-opportunity-in-machine.html).\n",
    "- Analyze ML fairness with [this walkthrough](https://pair-code.github.io/what-if-tool/learn/tutorials/walkthrough/) of the What-If Tool, created by the People and AI Research (PAIR) team at Google. This tool allows you to quickly amend an ML model, once you've picked the fairness criterion that is best for your use case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002878,
     "end_time": "2021-02-17T20:43:06.871617",
     "exception": false,
     "start_time": "2021-02-17T20:43:06.868739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your turn\n",
    "\n",
    "Continue to the exercise to **[run code](#$NEXT_NOTEBOOK_URL$)** to train models and determine fairness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.26116,
   "end_time": "2021-02-17T20:43:07.485248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-17T20:43:00.224088",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
